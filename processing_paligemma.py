
from typing import Dict, List, Optional, Union, Tuple, Iterable
import numpy as np
from PIL import Image
import torch

IMAGENET_STANDARD_MEAN = [0.5, 0.5, 0.5]
IMAGENET_STANDARD_STD = [0.5, 0.5, 0.5]

def resize(image: Image, size: Tuple[int, int], resample: Image.Resampling = None, reducing_gap: Optional[int] = None) -> np.ndarray:
  return image.resize(size=size, resample=resample, reducing_gap=reducing_gap)


def rescale(image: np.array, scale: float, dtype: np.dtype = np.float32) -> np.ndarray:
  return (image * scale).astype(dtype)


def normalize(image: np.array, mean: Union[float, Iterable[float]], std: Union[float, Iterable[float]]) -> np.ndarray:
  mean = np.array(mean, dtype=image.dtype)
  std = np.array(std, dtype=image.dtype)
  return (image - mean) / std


def process_image(
    images: List[Image.Image],
    size: Dict[str, int] = None,
    resample: Image.Resampling = None,
    rescale_factor: float = None,
    image_mean: Optional[Union[float, List[float]]] = None,
    image_std: Optional[Union[float, List[float]]] = None
) -> List[np.ndarray]:
  height, width = size[0], size[1]
  images = [resize(image=image, size=(height,width), resample=resample) for image in images]
  images = [np.array(image) for image in images]
  images = [rescale(image=image, scale=rescale_factor) for image in images]
  images = [normalize(image=image, mean=image_mean, std=image_std) for image in images]
  images = [image.transpose(2,0,1) for image in images]
  return images

def add_image_tokens_to_prompt(prefix_prompt, bos_token, image_seq_len, image_token):
  return f"{image_token * image_seq_len}{bos_token}{prefix_prompt}\n"



class PaliGemmaProcessor:
  IMAGE_TOKEN = "<image>"

  def __init__(self, tokenizer, num_image_tokens: int, image_size: int):
    super().__init__()

    self.num_image_tokens = num_image_tokens
    self.image_size = image_size

    tokens_to_add = {"additional_special_tokens": [self.IMAGE_TOKEN]}
    tokenizer.add_special_tokens(tokens_to_add)
    EXTRA_TOKENS = [f"<loc{i:04d}>" for i in range(1024)]
    EXTRA_TOKENS += [f"<loc{i:03d}>" for i in range(128)]
    tokenizer.add_tokens(EXTRA_TOKENS)
    self.image_token_id = tokenizer.convert_tokens_to_ids(self.IMAGE_TOKEN)
    tokenizer.add_bos_token = False
    tokenizer.add_eos_token = False

    self.tokenizer = tokenizer

  def __call__(self, text: List[str], images: List[Image.Image], padding: str = "longest", truncation: bool = True) -> dict:
    assert len(images) == 1 and len(text) == 1, f"Received {len(images)} images for {len(text)} prompts."

    pixel_values = process_image(images=images, size=self.image_size, resample=Image.Resampling.BICUBIC, rescale_factor=1/255.0, image_mean=IMAGENET_STANDARD_MEAN, image_std=IMAGENET_STANDARD_STD)
    pixel_values = np.stack(pixel_values, axis=0)
    pixel_values = torch.tensor(pixel_values)

    input_strings = [add_image_tokens_to_prompt(prefix_prompt=prompt, bos_token=self.tokenizer.bos_token, image_seq_len=num_image_tokens, image_token=self.IMAGE_TOKEN) for prompt in text]
    inputs = self.tokenizer(input_strings, return_tensors='pt', padding=padding, truncation=truncation)
    return_data = {"pixel_values": pixel_values, **inputs}

    return return_data

